[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Allen Bush-Beaupré",
    "section": "",
    "text": "A bit about me:\nI am a research professional based in Sherbrooke, QC, CAN. I currently work for Marc Bélisle at Université de Sherbrooke and for Jade Savage at Bishop’s University. My work can be summarized as trying to apply statistical methods to research questions pertaining to agricultural entomology.\nMy work for Jade Savage focuses on Delia flies (D. antiqua, D. radicum, D. platura & D. florilega) for the Delia Project."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "My Blog",
    "section": "",
    "text": "Visualizing indirect effects\n\n\n\ndata viz\n\n\nindirect effects\n\n\nmethod development\n\n\n\nHow do we quantify and visualize indirect effects in a path analysis when relationships are non-linear? Worse yet; non-monotonous?\n\n\n\nAllen Bush-Beaupré\n\n\nOct 31, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/indirect_effects/index.html",
    "href": "posts/indirect_effects/index.html",
    "title": "Visualizing indirect effects",
    "section": "",
    "text": "Start by loading the pertinent packages\n\n\nCode\nrm(list=ls()) # clear workspace\nlibrary(tidyverse) # data manipulation and plotting\nlibrary(glmmTMB) # frequentist glmms\nlibrary(patchwork) #multiple plots\nlibrary(ggdist) # nice dots in logistic regression\nlibrary(tidylog) # log changes done during data wrangling\nlibrary(mgcv) # gamms\nlibrary(gratia) # gam predictions\n\n\nFirst, we will simulate data from this DAG:\nINSERT DAG\n\n\nCode\nset.seed(333) # set seed to reproduce the simulations exactly\nn &lt;- 3000 # sample size\nX &lt;- rnorm(n, 0, 1) # define K as a Z-score\nY &lt;- rpois(n, exp(4 + 0.5*X)) # Y is a poisson variable whose link is the log function so defining the coefficients using the inverse log (exponential)\nK &lt;- rbinom(n, size = 1, prob = plogis(-3 + 0.03*X + 0.05*Y)) # K is a Bernoulli variable whose link is the logit so defining the coefficients using the inverse logit (plogis)\n\ndata &lt;- tibble(\n  X = X,\n  Y = Y,\n  K = K,\n  K_1 = 1 - K\n)\n\nhead(data)\n\n\n# A tibble: 6 × 4\n        X     Y     K   K_1\n    &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt;\n1 -0.0828    42     0     1\n2  1.93     151     1     0\n3 -2.05      20     0     1\n4  0.278     65     0     1\n5 -1.53      22     0     1\n6 -0.269     56     0     1\n\n\nNow, we want to model this path analysis\nModel for Y\n\n\nCode\nmY_X &lt;- glmmTMB(Y ~ X, family = poisson, data = data)\nsummary(mY_X)\n\n\n Family: poisson  ( log )\nFormula:          Y ~ X\nData: data\n\n     AIC      BIC   logLik deviance df.resid \n 20576.1  20588.1 -10286.1  20572.1     2998 \n\n\nConditional model:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) 3.997055   0.002606    1534   &lt;2e-16 ***\nX           0.499396   0.002281     219   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nModel for K\n\n\nCode\nmK_XY &lt;- glmmTMB(cbind(K, K_1) ~ X + Y, family = binomial, data = data)\nsummary(mK_XY)\n\n\n Family: binomial  ( logit )\nFormula:          cbind(K, K_1) ~ X + Y\nData: data\n\n     AIC      BIC   logLik deviance df.resid \n  3243.0   3261.0  -1618.5   3237.0     2997 \n\n\nConditional model:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -2.682855   0.284409  -9.433   &lt;2e-16 ***\nX            0.075036   0.133618   0.562    0.574    \nY            0.045318   0.004856   9.332   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nNotice the p-value for X\nExtract predictions of Y ~ X\nHere, I use a long-winded way to compute the predictions to show how it is done\n\n\nCode\n#create dataframe with values of X for which we want to predict Y\nnew_dat_Y_X &lt;- tibble(X = seq(from = min(data$X), to = max(data$X), length.out = 30)) \n\n#predict Y but on the link scale (log) to calculate confidence intervals\npred_new_dat_Y_X &lt;- predict(mY_X, newdata = new_dat_Y_X, type = \"link\", se.fit = TRUE)\n\n#extract the inverse link function for the model (exponential in this case) \nilink&lt;- family(mY_X)$linkinv\n# bind the dataframes with values of X and the predicted Y values\npredictions_Y_X &lt;- cbind(new_dat_Y_X,data.frame(pred_new_dat_Y_X)) %&gt;%\n  #calculate the confidence intervals on the log scale and exponentiate\n  mutate(CI.up = ilink(fit + (1.96*se.fit)),\n         CI.low = ilink(fit - (1.96*se.fit)),\n         Pred=ilink(fit))\n\n\nPlot\n\n\nCode\npY_X &lt;- ggplot() +\n  geom_line(data = predictions_Y_X, aes(y = Pred, x = X)) +\n  geom_ribbon(data = predictions_Y_X, aes(ymin = CI.low, ymax = CI.up, x = X)) +\n  geom_point(data = data, aes(x = X, y = Y), alpha = 0.2) +\n  theme_classic() +\n  labs(title = \"Effect of X on Y\")\n\npY_X\n\n\n\n\n\nNow, the conditional (direct) effect of X on K\n\n\nCode\nnew_dat_K_X &lt;- tibble(X = seq(from = min(data$X), to = max(data$X), length.out = 30),\n                      Y = mean(data$Y)) # set y to its mean to \n\npred_new_dat_K_X &lt;- predict(mK_XY, newdata = new_dat_K_X, type = \"link\", se.fit = TRUE)\n\nilink&lt;- family(mK_XY)$linkinv\npredictions_K_X &lt;- cbind(new_dat_K_X,data.frame(pred_new_dat_K_X)) %&gt;%\n  mutate(CI.up = ilink(fit + (1.96*se.fit)),\n         CI.low = ilink(fit - (1.96*se.fit)),\n         Pred=ilink(fit))\n\n\npK_X &lt;- ggplot() +\n  stat_dots(data = data,\n            aes(y = K,\n                x = X,\n                side = ifelse(K_1 == 0, \"bottom\", \"top\")),\n            size = 2)+\n  geom_line(data = predictions_K_X, aes(y = Pred, x = X)) +\n  geom_ribbon(data = predictions_K_X, aes(ymin = CI.low, ymax = CI.up, x = X)) +\n  \n  theme_classic() +\n  labs(title = \"Conditional effect of X on K\")\n\npK_X \n\n\n\n\n\nWhat about the conditional (direct) effect of Y on K ?\n\n\nCode\nnew_dat_K_Y &lt;- tibble(Y = seq(from = min(data$Y), to = max(data$Y), length.out = 30),\n                      X = mean(data$X)) # set X to its mean\n\npred_new_dat_K_Y &lt;- predict(mK_XY, newdata = new_dat_K_Y, type = \"link\", se.fit = TRUE)\n\nilink&lt;- family(mK_XY)$linkinv\npredictions_K_Y &lt;- cbind(new_dat_K_Y,data.frame(pred_new_dat_K_Y)) %&gt;%\n  mutate(CI.up = ilink(fit + (1.96*se.fit)),\n         CI.low = ilink(fit - (1.96*se.fit)),\n         Pred=ilink(fit))\n\npK_Y &lt;- ggplot() +\n  stat_dots(data = data,\n            aes(y = K,\n                x = Y,\n                side = ifelse(K_1 == 0, \"bottom\", \"top\")),\n            size = 2)+\n  geom_line(data = predictions_K_Y, aes(y = Pred, x = Y)) +\n  geom_ribbon(data = predictions_K_Y, aes(ymin = CI.low, ymax = CI.up, x = Y)) +\n  theme_classic() +\n  labs(title = \"Conditional effect of Y on K\")\n\npK_Y \n\n\n\n\n\nSo this is the direct effect of Y on K. But, remember that Y is caused by X so it may be of interest to see how X affects K indirectly, through Y\n\n\nCode\npY_X\n\n\n\n\n\nThe first step is to select values of Y that are predicted by X\n\n\nCode\nCOLS &lt;- alpha(colorRampPalette(c(\"blue\",\"red\"))(30),0.6)\npY_X + geom_point(data = predictions_Y_X, aes(y = Pred, x = X, color = X), size = 4) +scale_color_gradientn(colours = COLS)\n\n\n\n\n\nThe next step is actually quite simple. We generate predictions for K as a function of the predictions of Y ~ X\n\nnew_dat_K_YX &lt;- tibble(Y = predictions_Y_X$Pred, # predicted mean Y from X\n                      X = mean(data$X)) # set X to its mean\n\n\n\nCode\npred_new_dat_K_YX &lt;- predict(mK_XY, newdata = new_dat_K_YX, type = \"link\", se.fit = TRUE)\n\nilink&lt;- family(mK_XY)$linkinv\npredictions_K_YX &lt;- cbind(new_dat_K_YX,data.frame(pred_new_dat_K_YX)) %&gt;%\n  mutate(CI.up = ilink(fit + (1.96*se.fit)),\n         CI.low = ilink(fit - (1.96*se.fit)),\n         Pred=ilink(fit))\n\n\nNow, we need the values of X that were associated with the predicted values of Y\n\n\nCode\npred_K_YX &lt;- select(predictions_K_YX, c(Y, Pred)) %&gt;%\n  left_join(predictions_Y_X %&gt;% \n              select(X, Pred) %&gt;% \n              rename(Y = Pred), \n            by = \"Y\")\n\npred_K_YX\n\n\nNow, we simply superimpose both graphs of K ~ Y\n\n\nCode\npK_Y \n\n\n\n\n\n\n\nCode\npK_Y + geom_line(data = pred_K_YX, aes(x = Y, y = Pred, color = X), linewidth= 2) +\n  labs(title = \"Indirect effect of X on K through Y\")\n\n\n\n\n\n\n\n\nCitationBibTeX citation:@online{bush-beaupré2023,\n  author = {Bush-Beaupré, Allen},\n  title = {Visualizing Indirect Effects},\n  date = {2023-10-31},\n  url = {https://abushbeaupre.github.io/posts/indirect_effects/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBush-Beaupré, Allen. 2023. “Visualizing Indirect Effects.”\nOctober 31, 2023. https://abushbeaupre.github.io/posts/indirect_effects/."
  }
]