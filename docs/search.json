[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Allen Bush-Beaupré",
    "section": "",
    "text": "A bit about me:\nI am a research professional based in Sherbrooke, QC, CAN. I currently work for Marc Bélisle at Université de Sherbrooke and for Jade Savage at Bishop’s University. My work can be summarized as trying to apply statistical methods to research questions pertaining to agricultural entomology.\nMy work for Jade Savage focuses on Delia flies (D. antiqua, D. radicum, D. platura & D. florilega) for the Delia Project."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "My Blog",
    "section": "",
    "text": "Visualizing indirect effects\n\n\n\ndata viz\n\n\nindirect effects\n\n\nmethod development\n\n\n\nHow do we quantify and visualize indirect effects in a path analysis when relationships are non-linear? Worse yet; non-monotonous?\n\n\n\nAllen Bush-Beaupré\n\n\nOct 31, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/indirect_effects/index.html",
    "href": "posts/indirect_effects/index.html",
    "title": "Visualizing indirect effects",
    "section": "",
    "text": "When fitting a model (ex. glm) in a causal inference context, we are either estimating the direct or total effect of a given variable. For example, in a DAG such as this one:\n\nWe can estimate the direct effect of X on K by fitting a model such as\nlm(K ~ X + Y)\nIn this case, we estimate the direct effect of X on K as we are controlling for Y. Such is the nature of (generalized) linear models.\nIf we want to estimate the total effect of X on K, we simply fit the model\nlm(K ~ X)\nIn this case, as we are ignoring Y, we estimate the direct effect of X + its indirect effect through Y (the total effect).\nBut what if we are interested in looking at the indirect effects?\nAs François Briau once told me, the study of Ecology is the study of direct and indirect effects. We just don’t really do it formally.\nLuckily, Bill Shipley has spent quite a bit of brain power in quantifying direct, indirect and total effects which is what we call Path Analysis.\n\n\n\nThe great Bill Shipley\n\n\nVery briefly, to calculate indirect effects in along a path (ex. TSF -&gt; Sphagnum cover -&gt; forest floor thickness), one must multiply the (standardized) coefficients along that path. In this case, 0.073 * 0.164 = 0.011972\n\n\n\nN. Fenton et al. / Forest Ecology and Management 213 (2005) 151–159\n\n\nBut what does that number represent? When all relationships are linear, that value represents a slope. When the relationship(s) is/are not linear (ex. Poisson and logistic regressions), the value represents the slope on the link scale (log or logit or other). On the response scale, the slope is not constant through the entire relationship (we will see later). So how do we interpret such indirect effects if they are not on the scale of the variable we measured?\nStart by loading the pertinent packages\n\n\nCode\nrm(list=ls()) # clear workspace\nlibrary(tidyverse) # data manipulation and plotting\nlibrary(glmmTMB) # frequentist glmms\nlibrary(patchwork) #multiple plots\nlibrary(ggdist) # nice dots in logistic regression\nlibrary(tidylog) # log changes done during data wrangling\nlibrary(mgcv) # gamms\nlibrary(gratia) # gam predictions\n\n\nFirst, we will simulate data from this DAG:\n\nBut these are just letters - BORING\nThe DAG could also be\n\nor\n\nYou don`t have to think too hard to see this type of relationship in your own work.\n\n\nCode\nset.seed(333) # set seed to reproduce the simulations exactly\nn &lt;- 3000 # sample size\nX &lt;- rnorm(n, 0, 1) # define K as a Z-score\nY &lt;- rpois(n, exp(4 + 0.5*X)) # Y is a poisson variable whose link is the log function so defining the coefficients using the inverse log (exponential)\nK &lt;- rbinom(n, size = 1, prob = plogis(-3 + 0.03*X + 0.05*Y)) # K is a Bernoulli variable whose link is the logit so defining the coefficients using the inverse logit (plogis)\n\ndata &lt;- tibble(\n  X = X,\n  Y = Y,\n  K = K,\n  K_1 = 1 - K\n)\n\nhead(data)\n\n\n# A tibble: 6 × 4\n        X     Y     K   K_1\n    &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt;\n1 -0.0828    42     0     1\n2  1.93     151     1     0\n3 -2.05      20     0     1\n4  0.278     65     0     1\n5 -1.53      22     0     1\n6 -0.269     56     0     1\n\n\nNow, we want to model this path analysis\nModel for Y\n\n\nCode\nmY_X &lt;- glmmTMB(Y ~ X, family = poisson, data = data)\nsummary(mY_X)\n\n\n Family: poisson  ( log )\nFormula:          Y ~ X\nData: data\n\n     AIC      BIC   logLik deviance df.resid \n 20576.1  20588.1 -10286.1  20572.1     2998 \n\n\nConditional model:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) 3.997055   0.002606    1534   &lt;2e-16 ***\nX           0.499396   0.002281     219   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nModel for K\n\n\nCode\nmK_XY &lt;- glmmTMB(cbind(K, K_1) ~ X + Y, family = binomial, data = data)\nsummary(mK_XY)\n\n\n Family: binomial  ( logit )\nFormula:          cbind(K, K_1) ~ X + Y\nData: data\n\n     AIC      BIC   logLik deviance df.resid \n  3243.0   3261.0  -1618.5   3237.0     2997 \n\n\nConditional model:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -2.682855   0.284409  -9.433   &lt;2e-16 ***\nX            0.075036   0.133618   0.562    0.574    \nY            0.045318   0.004856   9.332   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nNotice the p-value for X\nExtract predictions of Y ~ X\nHere, I use a long-winded way to compute the predictions to show how it is done\n\n\nCode\n#create dataframe with values of X for which we want to predict Y\nnew_dat_Y_X &lt;- tibble(X = seq(from = min(data$X), to = max(data$X), length.out = 30)) \n\n#predict Y but on the link scale (log) to calculate confidence intervals\npred_new_dat_Y_X &lt;- predict(mY_X, newdata = new_dat_Y_X, type = \"link\", se.fit = TRUE)\n\n#extract the inverse link function for the model (exponential in this case) \nilink&lt;- family(mY_X)$linkinv\n# bind the dataframes with values of X and the predicted Y values\npredictions_Y_X &lt;- cbind(new_dat_Y_X,data.frame(pred_new_dat_Y_X)) %&gt;%\n  #calculate the confidence intervals on the log scale and exponentiate\n  mutate(CI.up = ilink(fit + (1.96*se.fit)),\n         CI.low = ilink(fit - (1.96*se.fit)),\n         Pred=ilink(fit))\n\n\nPlot\n\n\nCode\npY_X &lt;- ggplot() +\n  geom_line(data = predictions_Y_X, aes(y = Pred, x = X)) +\n  geom_ribbon(data = predictions_Y_X, aes(ymin = CI.low, ymax = CI.up, x = X)) +\n  geom_point(data = data, aes(x = X, y = Y), alpha = 0.2) +\n  theme_classic() +\n  labs(title = \"Effect of X on Y\")\n\npY_X\n\n\n\n\n\nNow, the conditional (direct) effect of X on K\n\n\nCode\nnew_dat_K_X &lt;- tibble(X = seq(from = min(data$X), to = max(data$X), length.out = 30),\n                      Y = mean(data$Y)) # set y to its mean to \n\npred_new_dat_K_X &lt;- predict(mK_XY, newdata = new_dat_K_X, type = \"link\", se.fit = TRUE)\n\nilink&lt;- family(mK_XY)$linkinv\npredictions_K_X &lt;- cbind(new_dat_K_X,data.frame(pred_new_dat_K_X)) %&gt;%\n  mutate(CI.up = ilink(fit + (1.96*se.fit)),\n         CI.low = ilink(fit - (1.96*se.fit)),\n         Pred=ilink(fit))\n\n\npK_X &lt;- ggplot() +\n  stat_dots(data = data,\n            aes(y = K,\n                x = X,\n                side = ifelse(K_1 == 0, \"bottom\", \"top\")),\n            size = 2)+\n  geom_line(data = predictions_K_X, aes(y = Pred, x = X)) +\n  geom_ribbon(data = predictions_K_X, aes(ymin = CI.low, ymax = CI.up, x = X)) +\n  \n  theme_classic() +\n  labs(title = \"Conditional effect of X on K\")\n\npK_X \n\n\n\n\n\nWhat about the conditional (direct) effect of Y on K ?\n\n\nCode\nnew_dat_K_Y &lt;- tibble(Y = seq(from = min(data$Y), to = max(data$Y), length.out = 30),\n                      X = mean(data$X)) # set X to its mean\n\npred_new_dat_K_Y &lt;- predict(mK_XY, newdata = new_dat_K_Y, type = \"link\", se.fit = TRUE)\n\nilink&lt;- family(mK_XY)$linkinv\npredictions_K_Y &lt;- cbind(new_dat_K_Y,data.frame(pred_new_dat_K_Y)) %&gt;%\n  mutate(CI.up = ilink(fit + (1.96*se.fit)),\n         CI.low = ilink(fit - (1.96*se.fit)),\n         Pred=ilink(fit))\n\npK_Y &lt;- ggplot() +\n  stat_dots(data = data,\n            aes(y = K,\n                x = Y,\n                side = ifelse(K_1 == 0, \"bottom\", \"top\")),\n            size = 2)+\n  geom_line(data = predictions_K_Y, aes(y = Pred, x = Y)) +\n  geom_ribbon(data = predictions_K_Y, aes(ymin = CI.low, ymax = CI.up, x = Y)) +\n  theme_classic() +\n  labs(title = \"Conditional effect of Y on K\")\n\npK_Y \n\n\n\n\n\nSo this is the direct effect of Y on K. But, remember that Y is caused by X so it may be of interest to see how X affects K indirectly, through Y\n\n\nCode\npY_X\n\n\n\n\n\nThe first step is to select values of Y that are predicted by X\n\n\nCode\nCOLS &lt;- alpha(colorRampPalette(c(\"blue\",\"red\"))(30),0.6)\npY_X + geom_point(data = predictions_Y_X, aes(y = Pred, x = X, color = X), size = 4) +scale_color_gradientn(colours = COLS)\n\n\n\n\n\nThe next step is actually quite simple. We generate predictions for K as a function of the predictions of Y ~ X\n\nnew_dat_K_YX &lt;- tibble(Y = predictions_Y_X$Pred, # predicted mean Y from X\n                      X = mean(data$X)) # set X to its mean\n\n\n\nCode\npred_new_dat_K_YX &lt;- predict(mK_XY, newdata = new_dat_K_YX, type = \"link\", se.fit = TRUE)\n\nilink&lt;- family(mK_XY)$linkinv\npredictions_K_YX &lt;- cbind(new_dat_K_YX,data.frame(pred_new_dat_K_YX)) %&gt;%\n  mutate(CI.up = ilink(fit + (1.96*se.fit)),\n         CI.low = ilink(fit - (1.96*se.fit)),\n         Pred=ilink(fit))\n\n\nNow, we need the values of X that were associated with the predicted values of Y\n\n\nCode\npred_K_YX &lt;- select(predictions_K_YX, c(Y, Pred)) %&gt;%\n  left_join(predictions_Y_X %&gt;% \n              select(X, Pred) %&gt;% \n              rename(Y = Pred), \n            by = \"Y\")\n\npred_K_YX\n\n\nNow, we simply superimpose both graphs of K ~ Y\n\n\nCode\npK_Y \n\n\n\n\n\n\n\nCode\npK_Y + geom_line(data = pred_K_YX, aes(x = Y, y = Pred, color = X), linewidth= 2) +\n  labs(title = \"Indirect effect of X on K through Y\") + scale_color_gradientn(colours = COLS)\n\n\n\n\n\nWe could push this even further and look at indirect effects through a non-monotonous relationship\n\nAnd what about interactions?\nSimulate data\n\n\nCode\nn &lt;- 3000\n#interacting variables\nX1 &lt;- rnorm(n = n, mean = 0, sd = 1)\nX2 &lt;- rpois(n = n, lambda = 3)\n#mediator\nY &lt;- rpois(n = n, lambda = exp(0 + 0.4*X1 + 0.3*X2 + 0.1*X1*X2))\n#outcome\nK &lt;- rbinom(n = n, size = 1, prob = plogis(-2 + 0.1*X1 + 0.2*X2 + 0.5*X1*X2 - 0.05*Y))\n\ndata &lt;- tibble(\n  X1 = X1,\n  X2 = X2,\n  Y = Y,\n  K = K,\n  K_1 = K - 1\n)\n\nhead(data)\n\n\n# A tibble: 6 × 5\n      X1    X2     Y     K   K_1\n   &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt;\n1  1.49      4    12     1     0\n2  0.582     4     6     0    -1\n3  1.30      1     3     0    -1\n4 -2.01      1     0     0    -1\n5  1.64      3     7     1     0\n6  0.260     2     3     1     0\n\n\nModels\n\n\nCode\nmY_X &lt;- glmmTMB(Y ~  X1 + X2 + X1*X2, family = poisson, data = data)\n\nmK_XY &lt;- glmmTMB(K ~ X1 + X2 + X1*X2 + Y, family = binomial, data = data)\n\n\nPredictions\nY ~ X\n\n\nCode\nnew_data_YX &lt;- expand_grid(X1 = c(-1, 0, 1),\n                           X2 = seq(from = min(data$X2), to = max(data$X2), length.out = 30))\n\n\npred_new_dat_Y_X &lt;- predict(mY_X, newdata = new_data_YX, type = \"link\", se.fit = TRUE) # predict on link scale\n\nilink&lt;- family(mY_X)$linkinv # extract inverse link (exponential in this case)\n\n#calculate confidence intervals on link scale and back transform\npredictions_Y_X &lt;- cbind(new_data_YX,data.frame(pred_new_dat_Y_X)) %&gt;%\n  mutate(CI.up = ilink(fit + (1.96*se.fit)),\n         CI.low = ilink(fit - (1.96*se.fit)),\n         Pred=ilink(fit))\n\nggplot() +\n  geom_ribbon(data = predictions_Y_X, aes(ymin = CI.low, ymax = CI.up, x = X2, fill = as.factor(X1)), alpha = 0.2) +\n  geom_line(data = predictions_Y_X, aes(y = Pred, x = X2, color = as.factor(X1))) +\n  labs(y = \"Y\",\n       title = \"Interacting effect of X1 & X2 on Y\") +\n  theme_classic()\n\n\n\n\n\nK ~ X | Y\n\n\nCode\nnew_data_KX &lt;- expand_grid(X1 = c(-1, 0, 1),\n                           X2 = seq(from = min(data$X2), to = max(data$X2), length.out = 30),\n                           Y = mean(data$Y))\n\n\npred_new_dat_K_X &lt;- predict(mK_XY, newdata = new_data_KX, type = \"link\", se.fit = TRUE) # predict on link scale\n\nilink&lt;- family(mK_XY)$linkinv # extract inverse link (inverse logit in this case)\n\n#calculate confidence intervals on link scale and back transform\npredictions_K_X &lt;- cbind(new_data_KX,data.frame(pred_new_dat_K_X)) %&gt;%\n  mutate(CI.up = ilink(fit + (1.96*se.fit)),\n         CI.low = ilink(fit - (1.96*se.fit)),\n         Pred=ilink(fit))\n\npK_X &lt;- ggplot() +\n  geom_ribbon(data = predictions_K_X, aes(ymin = CI.low, ymax = CI.up, x = X2, fill = as.factor(X1)), alpha = 0.2) +\n  geom_line(data = predictions_K_X, aes(y = Pred, x = X2, color = as.factor(X1)))+\n  labs(y = \"K\",\n       title = \"Interacting effect of X1 & X2 on K\",\n       fill = \"X1\",\n       color = \"X1\") +\n  theme_classic()+theme(legend.position = c(0.1, 0.5))\n\npK_X\n\n\n\n\n\nK ~ Y | X\n\n\nCode\nnew_data_KY &lt;- expand_grid(X1 = mean(data$X1),\n                           X2 = mean(data$X2),\n                           Y = seq(from = min(data$Y), to = max(data$Y), length.out = 30))\n\n\npred_new_dat_K_Y &lt;- predict(mK_XY, newdata = new_data_KY, type = \"link\", se.fit = TRUE) # predict on link scale\n\nilink&lt;- family(mK_XY)$linkinv # extract inverse link (inverse logit in this case)\n\n#calculate confidence intervals on link scale and back transform\npredictions_K_Y &lt;- cbind(new_data_KY,data.frame(pred_new_dat_K_Y)) %&gt;%\n  mutate(CI.up = ilink(fit + (1.96*se.fit)),\n         CI.low = ilink(fit - (1.96*se.fit)),\n         Pred=ilink(fit))\n\nggplot() +\n  geom_ribbon(data = predictions_K_Y, aes(ymin = CI.low, ymax = CI.up, x = Y), alpha = 0.2) +\n  geom_line(data = predictions_K_Y, aes(y = Pred, x = Y))+\n  labs(y = \"K\",\n       title = \"Effect of Y on K\") +\n  theme_classic()\n\n\n\n\n\nK ~ E(Y|X)\n\n\nCode\npY_X &lt;- ggplot() +\n  geom_ribbon(data = predictions_Y_X, aes(ymin = CI.low, ymax = CI.up, x = X2, fill = as.factor(X1)), alpha = 0.2) +\n  geom_line(data = predictions_Y_X, aes(y = Pred, x = X2, color = as.factor(X1))) +\n  geom_point(data = predictions_Y_X, aes(y = Pred, x = X2, color = as.factor(X1))) +\n  labs(y = \"Y\",\n       title = \"Interacting effect of X1 & X2 on Y\",\n       color = \"X1\",\n       fill = \"X1\") +\n  theme_classic() +theme(legend.position = c(0.2, 0.5))\n\npY_X\n\n\n\n\n\nneed to predict K for the different predicted values of Y ~ X\n\n\nCode\nnew_data_KYX &lt;- tibble(\n  Y = predictions_Y_X$Pred,\n  X1 = mean(data$X1),\n  X2 = mean(data$X2)\n)\n\npred_new_dat_K_YX &lt;- predict(mK_XY, newdata = new_data_KYX, type = \"link\", se.fit = TRUE) # predict on link scale\n\nilink&lt;- family(mK_XY)$linkinv # extract inverse link (inverse logit in this case)\n\n#calculate confidence intervals on link scale and back transform\npredictions_K_YX &lt;- cbind(new_data_KYX,data.frame(pred_new_dat_K_YX)) %&gt;%\n  mutate(CI.up = ilink(fit + (1.96*se.fit)),\n         CI.low = ilink(fit - (1.96*se.fit)),\n         Pred=ilink(fit)) %&gt;%\n  select(-c(X1, X2)) %&gt;% #remove X1 and X2 that are set to their means as we don't need them\n  mutate(X1 = predictions_Y_X$X1, \n         X2 = predictions_Y_X$X2)  # add the values of X1 and X2 that were used to predict Y\n  \nCOLS &lt;- alpha(colorRampPalette(c(\"blue\",\"red\"))(30),0.6)\nggplot() +\n  geom_ribbon(data = predictions_K_YX, aes(ymin = CI.low, ymax = CI.up, x = Y, fill = as.factor(X1)), alpha = 0.2) +\n  geom_line(data = predictions_K_YX, aes(y = Pred, x = Y,  color = X2))+\n  scale_color_gradientn(colours = COLS)+\n  labs(y = \"K\",\n       title = \"Effect of X1*X2 on K through Y\",\n       fill = \"X1\") +\n  theme_classic() +\n  facet_wrap(~as.factor(X1))\n\n\n\n\n\nMake it “pretty”\n\n\nCode\nfacet_labels_X1 &lt;- as_labeller(\n  c(\n    `-1` = \"X1 = -1\",\n    `0` = \"X1 = 0\",\n    `1` = \"X1 = 1\"\n  ))\npK_XY &lt;- ggplot() +\n  geom_ribbon(data = predictions_K_YX, aes(ymin = CI.low, ymax = CI.up, x = Y, fill = as.factor(X1)), alpha = 0.2) +\n  geom_line(data = predictions_K_YX, aes(y = Pred, x = Y,  color = X2), linewidth = 2)+\n  scale_color_gradientn(colours = COLS)+\n  labs(\n       title = \"Effect of X1*X2 on K through Y\") +\n  guides(fill = \"none\") +\n  theme_classic() +\n  facet_wrap(~as.factor(X1), labeller = facet_labels_X1)+\n  theme(legend.position = c(0.2, 0.5),\n        axis.line.y = element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks.y = element_blank(),\n        axis.title.y = element_blank())\n\npK_XY\n\n\n\n\n\npK_Y\n\n\nCode\npK_Y &lt;- ggplot() +\n  geom_ribbon(data = predictions_K_Y, aes(ymin = CI.low, ymax = CI.up, x = Y), alpha = 0.2) +\n  geom_line(data = predictions_K_Y, aes(y = Pred, x = Y))+\n  labs(y = \"K\",\n       title = \"Effect of Y on K\") +\n  coord_cartesian(xlim = c(0, 100)) +\n  theme_classic()\n\npK_Y\n\n\n\n\n\npatchwork\n\n\nCode\npatch_K_Y &lt;- pK_Y + pK_XY + plot_annotation(caption = \"X1 is a strong mediator of the indirect effect of X2 on K through Y\")\n\npatch_K_Y\n\n\n\n\n\nBring it all together\n\n\n\n\nCitationBibTeX citation:@online{bush-beaupré2023,\n  author = {Bush-Beaupré, Allen},\n  title = {Visualizing Indirect Effects},\n  date = {2023-10-31},\n  url = {https://abushbeaupre.github.io/posts/indirect_effects/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBush-Beaupré, Allen. 2023. “Visualizing Indirect Effects.”\nOctober 31, 2023. https://abushbeaupre.github.io/posts/indirect_effects/."
  }
]