[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Allen Bush-Beaupré",
    "section": "",
    "text": "English\n\n\nFrançais"
  },
  {
    "objectID": "index.html#bienvenue",
    "href": "index.html#bienvenue",
    "title": "Allen Bush-Beaupré",
    "section": "Bienvenue",
    "text": "Bienvenue\nJe suis professionnel de recherche à l’Université de Sherbrooke et à l’Université Bishop’s, Sherbrooke, QC, CAN. Durant ma maîtrise (2020-22), j’ai développé un intérêt pour les méthodes quantitatives que j’ai maintenant le privilège d’utiliser dans les différents rôles que j’occupe.\nJ’étudie l’entomologie agricole avec un focus sur la lutte intégrée par une meilleure compréhension de l’écologie des insectes. Mon travail dans le laboratoire de Jade Savage (Université Bishop’s) porte sur le projet Delia, étudiant les mouches des racines (Delia spp. (Diptera : Anthomyiidae)) en partenariat avec PRISME/Phytodata et leur programme de mouches stériles. Mon travail dans le laboratoire de Marc Bélisle (Université de Sherbrooke) consiste à appliquer des méthodes statistiques aux différents projets en cours dans le laboratoire (hirondelles bicolores et leurs ectoparasites, phénologie des plantes et des insectes, inventaires de papillons, niveaux de contamination par les pesticides). Mon projet principal pour ce rôle, cependant, porte sur un projet impliquant la lutte intégrée dans les cultures de laitue (cliquez ici pour voir notre récente mission scientifique en Europe).\nDepuis 2026, je suis également chargé d’assister les membres du Centre de Recherche en Écologie de l’Université de Sherbrooke (CREUS) avec leurs analyses statistiques. Durant ces consultations, j’apprendrai inévitablement de nouvelles méthodes et créerai divers tutoriels. Ceux-ci seront disponibles sur mon blog. Si vous êtes membre du CREUS et souhaitez réserver une consultation avec moi, veuillez suivre les instructions sur la page Consultations CREUS."
  },
  {
    "objectID": "posts/indirect_effects/index.html",
    "href": "posts/indirect_effects/index.html",
    "title": "Visualizing indirect effects",
    "section": "",
    "text": "When fitting a model (ex. glm) in a causal inference context, we are either estimating the direct or total effect of a given variable. For example, in a DAG such as this one:\n\nWe can estimate the direct effect of X on K by fitting a model such as\nlm(K ~ X + Y)\nIn this case, we estimate the direct effect of X on K as we are controlling for Y. Such is the nature of (generalized) linear models.\nIf we want to estimate the total effect of X on K, we simply fit the model\nlm(K ~ X)\nIn this case, as we are ignoring Y, we estimate the direct effect of X + its indirect effect through Y (the total effect).\nBut what if we are interested in looking at the indirect effects?\nAs François Briau once told me, the study of Ecology is the study of direct and indirect effects. We just don’t really do it formally.\nLuckily, Bill Shipley has spent quite a bit of brain power in quantifying direct, indirect and total effects which is what we call Path Analysis.\n\n\n\nThe great Bill Shipley\n\n\nVery briefly, to calculate indirect effects in along a path (ex. TSF -&gt; Sphagnum cover -&gt; forest floor thickness), one must multiply the (standardized) coefficients along that path. In this case, 0.073 * 0.164 = 0.011972\n\n\n\nN. Fenton et al. / Forest Ecology and Management 213 (2005) 151–159\n\n\nBut what does that number represent? When all relationships are linear, that value represents a slope. When the relationship(s) is/are not linear (ex. Poisson and logistic regressions), the value represents the slope on the link scale (log or logit or other). On the response scale, the slope is not constant through the entire relationship (we will see later). So how do we interpret such indirect effects if they are not on the scale of the variable we measured?\nStart by loading the pertinent packages\n\n\nCode\nrm(list=ls()) # clear workspace\nlibrary(tidyverse) # data manipulation and plotting\nlibrary(glmmTMB) # frequentist glmms\nlibrary(patchwork) #multiple plots\nlibrary(ggdist) # nice dots in logistic regression\nlibrary(tidylog) # log changes done during data wrangling\nlibrary(mgcv) # gamms\nlibrary(gratia) # gam predictions\n\n\nFirst, we will simulate data from this DAG:\n\nBut these are just letters - BORING\nThe DAG could also be\n\nor\n\nYou don`t have to think too hard to see this type of relationship in your own work.\n\n\nCode\nset.seed(333) # set seed to reproduce the simulations exactly\nn &lt;- 3000 # sample size\nX &lt;- rnorm(n, 0, 1) # define K as a Z-score\nY &lt;- rpois(n, exp(4 + 0.5*X)) # Y is a poisson variable whose link is the log function so defining the coefficients using the inverse log (exponential)\nK &lt;- rbinom(n, size = 1, prob = plogis(-3 + 0.03*X + 0.05*Y)) # K is a Bernoulli variable whose link is the logit so defining the coefficients using the inverse logit (plogis)\n\ndata &lt;- tibble(\n  X = X,\n  Y = Y,\n  K = K,\n  K_1 = 1 - K\n)\n\nhead(data)\n\n\n# A tibble: 6 × 4\n        X     Y     K   K_1\n    &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt;\n1 -0.0828    42     0     1\n2  1.93     151     1     0\n3 -2.05      20     0     1\n4  0.278     65     0     1\n5 -1.53      22     0     1\n6 -0.269     56     0     1\n\n\nNow, we want to model this path analysis\nModel for Y\n\n\nCode\nmY_X &lt;- glmmTMB(Y ~ X, family = poisson, data = data)\nsummary(mY_X)\n\n\n Family: poisson  ( log )\nFormula:          Y ~ X\nData: data\n\n      AIC       BIC    logLik -2*log(L)  df.resid \n  20576.1   20588.1  -10286.1   20572.1      2998 \n\n\nConditional model:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) 3.997055   0.002606    1534   &lt;2e-16 ***\nX           0.499396   0.002281     219   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nModel for K\n\n\nCode\nmK_XY &lt;- glmmTMB(cbind(K, K_1) ~ X + Y, family = binomial, data = data)\nsummary(mK_XY)\n\n\n Family: binomial  ( logit )\nFormula:          cbind(K, K_1) ~ X + Y\nData: data\n\n      AIC       BIC    logLik -2*log(L)  df.resid \n   3243.0    3261.0   -1618.5    3237.0      2997 \n\n\nConditional model:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -2.682855   0.284409  -9.433   &lt;2e-16 ***\nX            0.075036   0.133618   0.562    0.574    \nY            0.045318   0.004856   9.332   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nNotice the p-value for X\nExtract predictions of Y ~ X\nHere, I use a long-winded way to compute the predictions to show how it is done\n\n\nCode\n#create dataframe with values of X for which we want to predict Y\nnew_dat_Y_X &lt;- tibble(X = seq(from = min(data$X), to = max(data$X), length.out = 30)) \n\n#predict Y but on the link scale (log) to calculate confidence intervals\npred_new_dat_Y_X &lt;- predict(mY_X, newdata = new_dat_Y_X, type = \"link\", se.fit = TRUE)\n\n#extract the inverse link function for the model (exponential in this case) \nilink&lt;- family(mY_X)$linkinv\n# bind the dataframes with values of X and the predicted Y values\npredictions_Y_X &lt;- cbind(new_dat_Y_X,data.frame(pred_new_dat_Y_X)) %&gt;%\n  #calculate the confidence intervals on the log scale and exponentiate\n  mutate(CI.up = ilink(fit + (1.96*se.fit)),\n         CI.low = ilink(fit - (1.96*se.fit)),\n         Pred=ilink(fit))\n\n\nPlot\n\n\nCode\npY_X &lt;- ggplot() +\n  geom_line(data = predictions_Y_X, aes(y = Pred, x = X)) +\n  geom_ribbon(data = predictions_Y_X, aes(ymin = CI.low, ymax = CI.up, x = X)) +\n  geom_point(data = data, aes(x = X, y = Y), alpha = 0.2) +\n  theme_classic() +\n  labs(title = \"Effect of X on Y\")\n\npY_X\n\n\n\n\n\n\n\n\n\nNow, the conditional (direct) effect of X on K\n\n\nCode\nnew_dat_K_X &lt;- tibble(X = seq(from = min(data$X), to = max(data$X), length.out = 30),\n                      Y = mean(data$Y)) # set y to its mean to \n\npred_new_dat_K_X &lt;- predict(mK_XY, newdata = new_dat_K_X, type = \"link\", se.fit = TRUE)\n\nilink&lt;- family(mK_XY)$linkinv\npredictions_K_X &lt;- cbind(new_dat_K_X,data.frame(pred_new_dat_K_X)) %&gt;%\n  mutate(CI.up = ilink(fit + (1.96*se.fit)),\n         CI.low = ilink(fit - (1.96*se.fit)),\n         Pred=ilink(fit))\n\n\npK_X &lt;- ggplot() +\n  stat_dots(data = data,\n            aes(y = K,\n                x = X,\n                side = ifelse(K_1 == 0, \"bottom\", \"top\")),\n            size = 2)+\n  geom_line(data = predictions_K_X, aes(y = Pred, x = X)) +\n  geom_ribbon(data = predictions_K_X, aes(ymin = CI.low, ymax = CI.up, x = X)) +\n  \n  theme_classic() +\n  labs(title = \"Conditional effect of X on K\")\n\npK_X \n\n\n\n\n\n\n\n\n\nWhat about the conditional (direct) effect of Y on K ?\n\n\nCode\nnew_dat_K_Y &lt;- tibble(Y = seq(from = min(data$Y), to = max(data$Y), length.out = 30),\n                      X = mean(data$X)) # set X to its mean\n\npred_new_dat_K_Y &lt;- predict(mK_XY, newdata = new_dat_K_Y, type = \"link\", se.fit = TRUE)\n\nilink&lt;- family(mK_XY)$linkinv\npredictions_K_Y &lt;- cbind(new_dat_K_Y,data.frame(pred_new_dat_K_Y)) %&gt;%\n  mutate(CI.up = ilink(fit + (1.96*se.fit)),\n         CI.low = ilink(fit - (1.96*se.fit)),\n         Pred=ilink(fit))\n\npK_Y &lt;- ggplot() +\n  stat_dots(data = data,\n            aes(y = K,\n                x = Y,\n                side = ifelse(K_1 == 0, \"bottom\", \"top\")),\n            size = 2)+\n  geom_line(data = predictions_K_Y, aes(y = Pred, x = Y)) +\n  geom_ribbon(data = predictions_K_Y, aes(ymin = CI.low, ymax = CI.up, x = Y)) +\n  theme_classic() +\n  labs(title = \"Conditional effect of Y on K\")\n\npK_Y \n\n\n\n\n\n\n\n\n\nSo this is the direct effect of Y on K. But, remember that Y is caused by X so it may be of interest to see how X affects K indirectly, through Y\n\n\nCode\npY_X\n\n\n\n\n\n\n\n\n\nThe first step is to select values of Y that are predicted by X\n\n\nCode\nCOLS &lt;- alpha(colorRampPalette(c(\"blue\",\"red\"))(30),0.6)\npY_X + geom_point(data = predictions_Y_X, aes(y = Pred, x = X, color = X), size = 4) +scale_color_gradientn(colours = COLS)\n\n\n\n\n\n\n\n\n\nThe next step is actually quite simple. We generate predictions for K as a function of the predictions of Y ~ X\n\nnew_dat_K_YX &lt;- tibble(Y = predictions_Y_X$Pred, # predicted mean Y from X\n                      X = mean(data$X)) # set X to its mean\n\n\n\nCode\npred_new_dat_K_YX &lt;- predict(mK_XY, newdata = new_dat_K_YX, type = \"link\", se.fit = TRUE)\n\nilink&lt;- family(mK_XY)$linkinv\npredictions_K_YX &lt;- cbind(new_dat_K_YX,data.frame(pred_new_dat_K_YX)) %&gt;%\n  mutate(CI.up = ilink(fit + (1.96*se.fit)),\n         CI.low = ilink(fit - (1.96*se.fit)),\n         Pred=ilink(fit))\n\n\nNow, we need the values of X that were associated with the predicted values of Y\n\n\nCode\npred_K_YX &lt;- select(predictions_K_YX, c(Y, Pred)) %&gt;%\n  left_join(predictions_Y_X %&gt;% \n              select(X, Pred) %&gt;% \n              rename(Y = Pred), \n            by = \"Y\")\n\npred_K_YX\n\n\nNow, we simply superimpose both graphs of K ~ Y\n\n\nCode\npK_Y \n\n\n\n\n\n\n\n\n\n\n\nCode\npK_Y + geom_line(data = pred_K_YX, aes(x = Y, y = Pred, color = X), linewidth= 2) +\n  labs(title = \"Indirect effect of X on K through Y\") + scale_color_gradientn(colours = COLS)\n\n\n\n\n\n\n\n\n\nWe could push this even further and look at indirect effects through a non-monotonous relationship\n\nAnd what about interactions?\nSimulate data\n\n\nCode\nn &lt;- 3000\n#interacting variables\nX1 &lt;- rnorm(n = n, mean = 0, sd = 1)\nX2 &lt;- rpois(n = n, lambda = 3)\n#mediator\nY &lt;- rpois(n = n, lambda = exp(0 + 0.4*X1 + 0.3*X2 + 0.1*X1*X2))\n#outcome\nK &lt;- rbinom(n = n, size = 1, prob = plogis(-2 + 0.1*X1 + 0.2*X2 + 0.5*X1*X2 - 0.05*Y))\n\ndata &lt;- tibble(\n  X1 = X1,\n  X2 = X2,\n  Y = Y,\n  K = K,\n  K_1 = K - 1\n)\n\nhead(data)\n\n\n# A tibble: 6 × 5\n      X1    X2     Y     K   K_1\n   &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt;\n1  1.49      4    12     1     0\n2  0.582     4     6     0    -1\n3  1.30      1     3     0    -1\n4 -2.01      1     0     0    -1\n5  1.64      3     7     1     0\n6  0.260     2     3     1     0\n\n\nModels\n\n\nCode\nmY_X &lt;- glmmTMB(Y ~  X1 + X2 + X1*X2, family = poisson, data = data)\n\nmK_XY &lt;- glmmTMB(K ~ X1 + X2 + X1*X2 + Y, family = binomial, data = data)\n\n\nPredictions\nY ~ X\n\n\nCode\nnew_data_YX &lt;- expand_grid(X1 = c(-1, 0, 1),\n                           X2 = seq(from = min(data$X2), to = max(data$X2), length.out = 30))\n\n\npred_new_dat_Y_X &lt;- predict(mY_X, newdata = new_data_YX, type = \"link\", se.fit = TRUE) # predict on link scale\n\nilink&lt;- family(mY_X)$linkinv # extract inverse link (exponential in this case)\n\n#calculate confidence intervals on link scale and back transform\npredictions_Y_X &lt;- cbind(new_data_YX,data.frame(pred_new_dat_Y_X)) %&gt;%\n  mutate(CI.up = ilink(fit + (1.96*se.fit)),\n         CI.low = ilink(fit - (1.96*se.fit)),\n         Pred=ilink(fit))\n\nggplot() +\n  geom_ribbon(data = predictions_Y_X, aes(ymin = CI.low, ymax = CI.up, x = X2, fill = as.factor(X1)), alpha = 0.2) +\n  geom_line(data = predictions_Y_X, aes(y = Pred, x = X2, color = as.factor(X1))) +\n  labs(y = \"Y\",\n       title = \"Interacting effect of X1 & X2 on Y\") +\n  theme_classic()\n\n\n\n\n\n\n\n\n\nK ~ X | Y\n\n\nCode\nnew_data_KX &lt;- expand_grid(X1 = c(-1, 0, 1),\n                           X2 = seq(from = min(data$X2), to = max(data$X2), length.out = 30),\n                           Y = mean(data$Y))\n\n\npred_new_dat_K_X &lt;- predict(mK_XY, newdata = new_data_KX, type = \"link\", se.fit = TRUE) # predict on link scale\n\nilink&lt;- family(mK_XY)$linkinv # extract inverse link (inverse logit in this case)\n\n#calculate confidence intervals on link scale and back transform\npredictions_K_X &lt;- cbind(new_data_KX,data.frame(pred_new_dat_K_X)) %&gt;%\n  mutate(CI.up = ilink(fit + (1.96*se.fit)),\n         CI.low = ilink(fit - (1.96*se.fit)),\n         Pred=ilink(fit))\n\npK_X &lt;- ggplot() +\n  geom_ribbon(data = predictions_K_X, aes(ymin = CI.low, ymax = CI.up, x = X2, fill = as.factor(X1)), alpha = 0.2) +\n  geom_line(data = predictions_K_X, aes(y = Pred, x = X2, color = as.factor(X1)))+\n  labs(y = \"K\",\n       title = \"Interacting effect of X1 & X2 on K\",\n       fill = \"X1\",\n       color = \"X1\") +\n  theme_classic()+theme(legend.position = c(0.1, 0.5))\n\npK_X\n\n\n\n\n\n\n\n\n\nK ~ Y | X\n\n\nCode\nnew_data_KY &lt;- expand_grid(X1 = mean(data$X1),\n                           X2 = mean(data$X2),\n                           Y = seq(from = min(data$Y), to = max(data$Y), length.out = 30))\n\n\npred_new_dat_K_Y &lt;- predict(mK_XY, newdata = new_data_KY, type = \"link\", se.fit = TRUE) # predict on link scale\n\nilink&lt;- family(mK_XY)$linkinv # extract inverse link (inverse logit in this case)\n\n#calculate confidence intervals on link scale and back transform\npredictions_K_Y &lt;- cbind(new_data_KY,data.frame(pred_new_dat_K_Y)) %&gt;%\n  mutate(CI.up = ilink(fit + (1.96*se.fit)),\n         CI.low = ilink(fit - (1.96*se.fit)),\n         Pred=ilink(fit))\n\nggplot() +\n  geom_ribbon(data = predictions_K_Y, aes(ymin = CI.low, ymax = CI.up, x = Y), alpha = 0.2) +\n  geom_line(data = predictions_K_Y, aes(y = Pred, x = Y))+\n  labs(y = \"K\",\n       title = \"Effect of Y on K\") +\n  theme_classic()\n\n\n\n\n\n\n\n\n\nK ~ E(Y|X)\n\n\nCode\npY_X &lt;- ggplot() +\n  geom_ribbon(data = predictions_Y_X, aes(ymin = CI.low, ymax = CI.up, x = X2, fill = as.factor(X1)), alpha = 0.2) +\n  geom_line(data = predictions_Y_X, aes(y = Pred, x = X2, color = as.factor(X1))) +\n  geom_point(data = predictions_Y_X, aes(y = Pred, x = X2, color = as.factor(X1))) +\n  labs(y = \"Y\",\n       title = \"Interacting effect of X1 & X2 on Y\",\n       color = \"X1\",\n       fill = \"X1\") +\n  theme_classic() +theme(legend.position = c(0.2, 0.5))\n\npY_X\n\n\n\n\n\n\n\n\n\nneed to predict K for the different predicted values of Y ~ X\n\n\nCode\nnew_data_KYX &lt;- tibble(\n  Y = predictions_Y_X$Pred,\n  X1 = mean(data$X1),\n  X2 = mean(data$X2)\n)\n\npred_new_dat_K_YX &lt;- predict(mK_XY, newdata = new_data_KYX, type = \"link\", se.fit = TRUE) # predict on link scale\n\nilink&lt;- family(mK_XY)$linkinv # extract inverse link (inverse logit in this case)\n\n#calculate confidence intervals on link scale and back transform\npredictions_K_YX &lt;- cbind(new_data_KYX,data.frame(pred_new_dat_K_YX)) %&gt;%\n  mutate(CI.up = ilink(fit + (1.96*se.fit)),\n         CI.low = ilink(fit - (1.96*se.fit)),\n         Pred=ilink(fit)) %&gt;%\n  select(-c(X1, X2)) %&gt;% #remove X1 and X2 that are set to their means as we don't need them\n  mutate(X1 = predictions_Y_X$X1, \n         X2 = predictions_Y_X$X2)  # add the values of X1 and X2 that were used to predict Y\n  \nCOLS &lt;- alpha(colorRampPalette(c(\"blue\",\"red\"))(30),0.6)\nggplot() +\n  geom_ribbon(data = predictions_K_YX, aes(ymin = CI.low, ymax = CI.up, x = Y, fill = as.factor(X1)), alpha = 0.2) +\n  geom_line(data = predictions_K_YX, aes(y = Pred, x = Y,  color = X2))+\n  scale_color_gradientn(colours = COLS)+\n  labs(y = \"K\",\n       title = \"Effect of X1*X2 on K through Y\",\n       fill = \"X1\") +\n  theme_classic() +\n  facet_wrap(~as.factor(X1))\n\n\n\n\n\n\n\n\n\nMake it “pretty”\n\n\nCode\nfacet_labels_X1 &lt;- as_labeller(\n  c(\n    `-1` = \"X1 = -1\",\n    `0` = \"X1 = 0\",\n    `1` = \"X1 = 1\"\n  ))\npK_XY &lt;- ggplot() +\n  geom_ribbon(data = predictions_K_YX, aes(ymin = CI.low, ymax = CI.up, x = Y, fill = as.factor(X1)), alpha = 0.2) +\n  geom_line(data = predictions_K_YX, aes(y = Pred, x = Y,  color = X2), linewidth = 2)+\n  scale_color_gradientn(colours = COLS)+\n  labs(\n       title = \"Effect of X1*X2 on K through Y\") +\n  guides(fill = \"none\") +\n  theme_classic() +\n  facet_wrap(~as.factor(X1), labeller = facet_labels_X1)+\n  theme(legend.position = c(0.2, 0.5),\n        axis.line.y = element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks.y = element_blank(),\n        axis.title.y = element_blank())\n\npK_XY\n\n\n\n\n\n\n\n\n\npK_Y\n\n\nCode\npK_Y &lt;- ggplot() +\n  geom_ribbon(data = predictions_K_Y, aes(ymin = CI.low, ymax = CI.up, x = Y), alpha = 0.2) +\n  geom_line(data = predictions_K_Y, aes(y = Pred, x = Y))+\n  labs(y = \"K\",\n       title = \"Effect of Y on K\") +\n  coord_cartesian(xlim = c(0, 100)) +\n  theme_classic()\n\npK_Y\n\n\n\n\n\n\n\n\n\npatchwork\n\n\nCode\npatch_K_Y &lt;- pK_Y + pK_XY + plot_annotation(caption = \"X1 is a strong mediator of the indirect effect of X2 on K through Y\")\n\npatch_K_Y\n\n\n\n\n\n\n\n\n\nBring it all together\n\n\n\n\nCitationBibTeX citation:@online{bush-beaupré2023,\n  author = {Bush-Beaupré, Allen},\n  title = {Visualizing Indirect Effects},\n  date = {2023-10-31},\n  url = {https://abushbeaupre.github.io/posts/indirect_effects/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBush-Beaupré, Allen. 2023. “Visualizing Indirect Effects.”\nOctober 31, 2023. https://abushbeaupre.github.io/posts/indirect_effects/."
  },
  {
    "objectID": "code_structure_tutorial.html",
    "href": "code_structure_tutorial.html",
    "title": "How to Structure Your Code for Statistical Consultations",
    "section": "",
    "text": "This document provides a template for structuring your R code when sharing analyses for statistical consultation. A well-organized document helps me understand your research question, data, and analysis quickly, allowing me to provide better feedback.\n\n\n\nClear introduction stating your research question\nLoad packages at the beginning\nData exploration before analysis\nCommented code explaining your thinking\nInterpret results in plain language"
  },
  {
    "objectID": "code_structure_tutorial.html#introduction",
    "href": "code_structure_tutorial.html#introduction",
    "title": "How to Structure Your Code for Statistical Consultations",
    "section": "",
    "text": "This document provides a template for structuring your R code when sharing analyses for statistical consultation. A well-organized document helps me understand your research question, data, and analysis quickly, allowing me to provide better feedback.\n\n\n\nClear introduction stating your research question\nLoad packages at the beginning\nData exploration before analysis\nCommented code explaining your thinking\nInterpret results in plain language"
  },
  {
    "objectID": "code_structure_tutorial.html#what-are-quarto-and-r-markdown",
    "href": "code_structure_tutorial.html#what-are-quarto-and-r-markdown",
    "title": "How to Structure Your Code for Statistical Consultations",
    "section": "What are Quarto and R Markdown?",
    "text": "What are Quarto and R Markdown?\nQuarto and R Markdown are document formats that combine text (like a Word document) with executable R code. They’re perfect for reproducible analyses because:\n\nYour code and explanations live in the same document\nYou can re-run analyses with one click\nOutputs (plots, tables, statistics) are automatically updated\nYou can share as HTML, PDF, or Word documents\n\n\nQuarto vs R Markdown - Which Should I Use?\nBoth work great! The main differences:\n\nR Markdown (.Rmd): The older, well-established format - requires rmarkdown package\nQuarto (.qmd): The newer format - works with R, Python, Julia, and more - doesn’t require extra R packages\n\nFor consultations, either format is fine! Use whichever you’re comfortable with.\n\n\nHow to Create a Quarto/R Markdown Document\nIn RStudio:\n\nFile → New File → Quarto Document (or R Markdown)\nGive it a title and author\nChoose HTML as output format\nClick “Create”\n\n\n\nHow to Render to HTML\nOnce you’ve written your document, you need to “render” it to create the HTML output:\nMethod 1: Click the Render/Knit button - Quarto: Click the “Render” button at the top of the editor - R Markdown: Click the “Knit” button at the top of the editor\nMethod 2: Use the console\n# For Quarto documents (.qmd)\nquarto::quarto_render(\"your_file.qmd\")\n\n# For R Markdown documents (.Rmd)\nrmarkdown::render(\"your_file.Rmd\")\nMethod 3: Use the terminal\n# For Quarto documents\nquarto render your_file.qmd\n\n\nWhat to Send Me\nWhen sharing your analysis for consultation, please send me both:\n\nThe source file (.qmd or .Rmd) - so I can see your code\nThe rendered HTML (.html) - so I can view your results without running the code\n\nThis way, I can review your outputs and dive into your code if needed!\n\n\nBasic Document Structure\nHere’s a minimal template to get you started:\n---\ntitle: \"My Analysis Title\"\nauthor: \"Your Name\"\ndate: \"2026-01-12\"\nformat: html\n---\n\n## Introduction\n\nWhat is your research question?\n\n## Load Packages\n\n```{r}\nlibrary(tidyverse)\n```\n\n## Load Data\n\n```{r}\nmy_data &lt;- read_csv(\"data.csv\")\n```\n\n## Analysis\n\n```{r}\n# Your analysis code here\n```\n\n## Results\n\nWhat do your results mean?"
  },
  {
    "objectID": "code_structure_tutorial.html#example-fiddler-crab-analysis",
    "href": "code_structure_tutorial.html#example-fiddler-crab-analysis",
    "title": "How to Structure Your Code for Statistical Consultations",
    "section": "Example: Fiddler Crab Analysis",
    "text": "Example: Fiddler Crab Analysis\nLet’s explore whether fiddler crabs follow Bergmann’s Rule - the pattern where organisms at higher latitudes are larger than those at lower latitudes.\n\nResearch Question\nDo Atlantic marsh fiddler crabs (Minuca pugnax) show larger body sizes at higher latitudes along the US Atlantic coast?\n\n\nLoad Required Packages\n\nlibrary(lterdatasampler)  # Contains the pie_crab dataset\nlibrary(tidyverse)         # For data manipulation and visualization\nlibrary(broom)            # For tidy model outputs\n\n\n\nLoad and Explore the Data\n\n# View the first few rows\nhead(pie_crab)\n\n# A tibble: 6 × 9\n  date       latitude site   size air_temp air_temp_sd water_temp water_temp_sd\n  &lt;date&gt;        &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;      &lt;dbl&gt;         &lt;dbl&gt;\n1 2016-07-24       30 GTM    12.4     21.8        6.39       24.5          6.12\n2 2016-07-24       30 GTM    14.2     21.8        6.39       24.5          6.12\n3 2016-07-24       30 GTM    14.5     21.8        6.39       24.5          6.12\n4 2016-07-24       30 GTM    12.9     21.8        6.39       24.5          6.12\n5 2016-07-24       30 GTM    12.4     21.8        6.39       24.5          6.12\n6 2016-07-24       30 GTM    13.0     21.8        6.39       24.5          6.12\n# ℹ 1 more variable: name &lt;chr&gt;\n\n# Check structure\nstr(pie_crab)\n\ntibble [392 × 9] (S3: tbl_df/tbl/data.frame)\n $ date         : Date[1:392], format: \"2016-07-24\" \"2016-07-24\" ...\n $ latitude     : num [1:392] 30 30 30 30 30 30 30 30 30 30 ...\n $ site         : chr [1:392] \"GTM\" \"GTM\" \"GTM\" \"GTM\" ...\n $ size         : num [1:392] 12.4 14.2 14.5 12.9 12.4 ...\n $ air_temp     : num [1:392] 21.8 21.8 21.8 21.8 21.8 ...\n $ air_temp_sd  : num [1:392] 6.39 6.39 6.39 6.39 6.39 ...\n $ water_temp   : num [1:392] 24.5 24.5 24.5 24.5 24.5 ...\n $ water_temp_sd: num [1:392] 6.12 6.12 6.12 6.12 6.12 ...\n $ name         : chr [1:392] \"Guana Tolomoto Matanzas NERR\" \"Guana Tolomoto Matanzas NERR\" \"Guana Tolomoto Matanzas NERR\" \"Guana Tolomoto Matanzas NERR\" ...\n\n# Summary statistics\nsummary(pie_crab)\n\n      date               latitude         site                size      \n Min.   :2016-07-24   Min.   :30.00   Length:392         Min.   : 6.64  \n 1st Qu.:2016-07-28   1st Qu.:34.00   Class :character   1st Qu.:12.02  \n Median :2016-08-01   Median :39.10   Mode  :character   Median :14.44  \n Mean   :2016-08-02   Mean   :37.69                      Mean   :14.66  \n 3rd Qu.:2016-08-09   3rd Qu.:41.60                      3rd Qu.:17.34  \n Max.   :2016-08-13   Max.   :42.70                      Max.   :23.43  \n    air_temp      air_temp_sd      water_temp    water_temp_sd  \n Min.   :10.29   Min.   :6.391   Min.   :13.98   Min.   :4.838  \n 1st Qu.:12.05   1st Qu.:8.110   1st Qu.:14.33   1st Qu.:6.567  \n Median :13.93   Median :8.410   Median :17.50   Median :6.998  \n Mean   :15.20   Mean   :8.654   Mean   :17.65   Mean   :7.252  \n 3rd Qu.:18.63   3rd Qu.:9.483   3rd Qu.:20.54   3rd Qu.:7.865  \n Max.   :21.79   Max.   :9.965   Max.   :24.50   Max.   :9.121  \n     name          \n Length:392        \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n\n\n\n\nData Exploration: Visualize Relationships\nBefore running statistical tests, always visualize your data!\n\nRelationship between latitude and water temperature\n\nggplot(data = pie_crab, aes(x = water_temp, y = latitude)) +\n  geom_point() +\n  labs(x = \"Water Temperature (°C)\",\n       y = \"Latitude\",\n       title = \"Water temperature decreases at higher latitudes\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nCrab size distribution by latitude\n\nggplot(data = pie_crab, aes(x = size, y = latitude, group = latitude)) +\n  geom_boxplot(aes(color = latitude)) +\n  labs(x = \"Carapace Width (mm)\",\n       y = \"Latitude\",\n       title = \"Fiddler crab size appears to increase with latitude\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nObservation: The boxplots suggest that crab size increases at higher latitudes, consistent with Bergmann’s Rule.\n\n\n\nStatistical Analysis\n\nSimple Linear Regression\nTesting if latitude predicts crab size:\n\n# Fit the model\ncrab_model &lt;- lm(size ~ latitude, data = pie_crab)\n\n# View results\nsummary(crab_model)\n\n\nCall:\nlm(formula = size ~ latitude, data = pie_crab)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.8376 -1.8797  0.1144  1.9484  6.9280 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -3.62442    1.27405  -2.845  0.00468 ** \nlatitude     0.48512    0.03359  14.441  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.832 on 390 degrees of freedom\nMultiple R-squared:  0.3484,    Adjusted R-squared:  0.3467 \nF-statistic: 208.5 on 1 and 390 DF,  p-value: &lt; 2.2e-16\n\n# Tidy output (optional but nice!)\ntidy(crab_model)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   -3.62     1.27       -2.84 4.68e- 3\n2 latitude       0.485    0.0336     14.4  3.60e-38\n\n\n\n\nModel Diagnostics\nAlways check your model assumptions!\n\npar(mfrow = c(2, 2))  # 2x2 plot layout\nplot(crab_model)\n\n\n\n\n\n\n\n\n\n\n\nResults Interpretation\n\n# Extract key statistics\nmodel_stats &lt;- glance(crab_model)\ncoef_stats &lt;- tidy(crab_model)\n\n# Print in plain language\ncat(\"Model Results:\\n\")\n\nModel Results:\n\ncat(\"- Slope:\", round(coef_stats$estimate[2], 3), \"mm per degree latitude\\n\")\n\n- Slope: 0.485 mm per degree latitude\n\ncat(\"- P-value:\", format(coef_stats$p.value[2], scientific = TRUE), \"\\n\")\n\n- P-value: 3.602897e-38 \n\ncat(\"- R-squared:\", round(model_stats$r.squared, 3), \"\\n\")\n\n- R-squared: 0.348 \n\n\nInterpretation: For each 1-degree increase in latitude, crab carapace width increases by approximately 0.49 mm on average (p &lt; 0.001). Latitude explains about 35% of the variation in crab size. This supports Bergmann’s Rule for this species.\n\n\nConclusions\n\nFiddler crabs show larger body sizes at higher latitudes along the US Atlantic coast\nThe relationship is statistically significant (p &lt; 0.001)\nHowever, latitude only explains ~35% of size variation - other factors likely matter\nModel diagnostics look reasonable (residuals appear normally distributed)\n\n\n\nNext Steps (Optional)\nIf I were to extend this analysis, I would:\n\nTest for non-linear relationships (e.g., quadratic terms)\nInclude water temperature as a predictor\nCheck for site-level effects (mixed models)\nExamine if the temperature-size rule applies"
  },
  {
    "objectID": "code_structure_tutorial.html#tips-for-your-own-code",
    "href": "code_structure_tutorial.html#tips-for-your-own-code",
    "title": "How to Structure Your Code for Statistical Consultations",
    "section": "Tips for Your Own Code",
    "text": "Tips for Your Own Code\n✅ DO:\n\nStart with a clear research question\nLoad all packages at the top\nComment your code liberally\nVisualize before analyzing\nCheck model assumptions\nInterpret results in plain language\n\n❌ DON’T:\n\nRun analyses without exploring the data first\nForget to check model diagnostics\nLeave code without comments\nSkip interpretation of statistical outputs\nSend code that doesn’t run!"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog",
    "section": "",
    "text": "Visualizing indirect effects\n\n\n\ndata viz\n\n\nindirect effects\n\n\nmethod development\n\n\n\nHow do we quantify and visualize indirect effects in a path analysis when relationships are non-linear? Worse yet; non-monotonous?\n\n\n\nAllen Bush-Beaupré\n\n\nOct 31, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "consultation.html#book-a-consultation",
    "href": "consultation.html#book-a-consultation",
    "title": "Consultations CREUS",
    "section": "Book a Consultation",
    "text": "Book a Consultation\nI provide statistical consulting services for graduate students and researchers members of the Centre de Recherche en Écologie de l’Université de Sherbrooke (CREUS). Whether you need help with study design, data analysis, or interpreting results, I’m here to assist.\n\nWhat I Can Help With\n\nStudy Design: Power analysis, sampling strategies, experimental design\nData Analysis: GLM/GLMM, path analysis, multivariate statistics, Bayesian methods\nR Programming: Data wrangling, visualization, reproducible workflows\nResults Interpretation: Understanding model outputs, effect sizes, diagnostics\n\n\n\nBefore Your Consultation\nPlease prepare:\n\nA brief description of your research question\nInformation about your data (sample size, variables, study design)\nSpecific questions or challenges you’re facing\nAny relevant code or output (if applicable)"
  },
  {
    "objectID": "consultation.html#réserver-une-consultation",
    "href": "consultation.html#réserver-une-consultation",
    "title": "Consultations CREUS",
    "section": "Réserver une consultation",
    "text": "Réserver une consultation\nJ’offre des services de consultation statistique aux étudiants diplômés et aux chercheurs membres du members of the Centre de Recherche en Écologie de l’Université de Sherbrooke (CREUS). Que vous ayez besoin d’aide pour la conception d’étude, l’analyse de données ou l’interprétation des résultats, je suis là pour vous aider.\n\nComment je peux aider\n\nConception d’étude: Analyse de puissance, stratégies d’échantillonnage, plan expérimental\nAnalyse de données: GLM/GLMM, GAM/GAMM, analyses de pistes, méthodes bayésiennes\nProgrammation R: Manipulation de données, visualisation, flux de travail reproductibles\nInterprétation des résultats: Comprendre les sorties de modèle, tailles d’effet, diagnostics\n\n\n\nAvant votre consultation\nVeuillez préparer:\n\nUne brève description de votre question de recherche\nInformations sur vos données (taille d’échantillon, variables, plan d’étude)\nQuestions spécifiques ou défis auxquels vous faites face\nTout code ou sortie pertinent (le cas échéant)"
  },
  {
    "objectID": "consultation.html#schedule-your-meeting",
    "href": "consultation.html#schedule-your-meeting",
    "title": "Consultations CREUS",
    "section": "Schedule Your Meeting",
    "text": "Schedule Your Meeting\nClick the button below to open the booking calendar in a new window. You’ll be able to answer a few questions about your needs and schedule a time that works for you. The intake questions will help me prepare for our meeting."
  },
  {
    "objectID": "consultation.html#planifier-votre-rencontre",
    "href": "consultation.html#planifier-votre-rencontre",
    "title": "Consultations CREUS",
    "section": "Planifier votre rencontre",
    "text": "Planifier votre rencontre\nCliquez sur le bouton ci-dessous pour ouvrir le calendrier de réservation dans une nouvelle fenêtre. Vous pourrez répondre à quelques questions sur vos besoins et planifier un moment qui vous convient. Les questions d’admission m’aideront à préparer notre rencontre."
  },
  {
    "objectID": "consultation.html#share-your-files",
    "href": "consultation.html#share-your-files",
    "title": "Consultations CREUS",
    "section": "Share Your Files",
    "text": "Share Your Files\nIf you have data files, code, or documents you’d like me to review before our meeting, you can upload them securely using the button below. Files will be saved directly to my Dropbox.\nWhat to share:\n\nData files (.csv, .xlsx, .txt, .rds, etc.)\nR scripts or analysis code\nOutput files or error messages\nResearch proposals or study designs\nAny other relevant documents"
  },
  {
    "objectID": "consultation.html#partager-vos-fichiers",
    "href": "consultation.html#partager-vos-fichiers",
    "title": "Consultations CREUS",
    "section": "Partager vos fichiers",
    "text": "Partager vos fichiers\nSi vous avez des fichiers de données, du code ou des documents que vous aimeriez que je révise avant notre rencontre, vous pouvez les télécharger en toute sécurité en utilisant le bouton ci-dessous. Les fichiers seront sauvegardés directement dans mon Dropbox.\nQuoi partager:\n\nFichiers de données (.csv, .xlsx, .txt, .rds, etc.)\nScripts R ou code d’analyse\nFichiers de sortie ou messages d’erreur\nPropositions de recherche ou plans d’étude\nTout autre document pertinent"
  },
  {
    "objectID": "consultation.html#sharing-your-code-with-me",
    "href": "consultation.html#sharing-your-code-with-me",
    "title": "Consultations CREUS",
    "section": "Sharing Your Code With Me",
    "text": "Sharing Your Code With Me\n\nHow to Structure Your Code\nA well-organized document helps me understand your research question, data, and analysis quickly, allowing me to provide better feedback.\nKey Elements of Good Code Structure:\n\nClear introduction stating your research question\nLoad packages at the beginning\nData exploration before analysis\nCommented code explaining your thinking\nInterpret results in plain language\n\n\n\nWhat are Quarto and R Markdown?\nQuarto and R Markdown are document formats that combine text (like a Word document) with executable R code. They’re perfect for reproducible analyses because:\n\nYour code and explanations live in the same document\nYou can re-run analyses with one click\nOutputs (plots, tables, statistics) are automatically updated\nYou can share as HTML, PDF, or Word documents\n\n\nQuarto vs R Markdown - Which Should I Use?\nBoth work great! The main differences:\n\nR Markdown (.Rmd): The older, well-established format - requires rmarkdown package\nQuarto (.qmd): The newer format - works with R, Python, Julia, and more - doesn’t require extra R packages\n\nFor consultations, either format is fine! Use whichever you’re comfortable with.\n\n\nHow to Create a Quarto/R Markdown Document\nIn RStudio:\n\nFile → New File → Quarto Document (or R Markdown)\nGive it a title and author\nChoose HTML as output format\nClick “Create”\n\n\n\nHow to Render to HTML\nOnce you’ve written your document, you need to “render” it to create the HTML output:\nMethod 1: Click the Render/Knit button - Quarto: Click the “Render” button at the top of the editor - R Markdown: Click the “Knit” button at the top of the editor\nMethod 2: Use the console\n# For Quarto documents (.qmd)\nquarto::quarto_render(\"your_file.qmd\")\n\n# For R Markdown documents (.Rmd)\nrmarkdown::render(\"your_file.Rmd\")\n\n\nWhat to Send Me\nWhen sharing your analysis for consultation, please send me both:\n\nThe source file (.qmd or .Rmd) - so I can see your code\nThe rendered HTML (.html) - so I can view your results without running the code\n\nThis way, I can review your outputs and dive into your code if needed!\n\n\n\nTips for Your Code\n✅ DO:\n\nStart with a clear research question\nLoad all packages at the top\nComment your code liberally\nVisualize before analyzing\nCheck model assumptions\nInterpret results in plain language\n\n❌ DON’T:\n\nRun analyses without exploring the data first\nForget to check model diagnostics\nLeave code without comments\nSkip interpretation of statistical outputs\nSend code that doesn’t run!\n\nFor a complete example of well-structured code, see the Code Structure Tutorial."
  },
  {
    "objectID": "consultation.html#partager-votre-code-avec-moi",
    "href": "consultation.html#partager-votre-code-avec-moi",
    "title": "Consultations CREUS",
    "section": "Partager votre code avec moi",
    "text": "Partager votre code avec moi\n\nComment structurer votre code\nUn document bien organisé m’aide à comprendre rapidement votre question de recherche, vos données et votre analyse, ce qui me permet de fournir de meilleurs commentaires.\nÉléments clés d’une bonne structure de code :\n\nIntroduction claire énonçant votre question de recherche\nCharger les packages au début\nExploration des données avant l’analyse\nCode commenté expliquant votre raisonnement\nInterpréter les résultats en langage simple\n\n\n\nQu’est-ce que Quarto et R Markdown?\nQuarto et R Markdown sont des formats de documents qui combinent du texte (comme un document Word) avec du code R exécutable. Ils sont parfaits pour les analyses reproductibles car :\n\nVotre code et vos explications sont dans le même document\nVous pouvez relancer les analyses en un clic\nLes résultats (graphiques, tableaux, statistiques) sont automatiquement mis à jour\nVous pouvez partager au format HTML, PDF ou Word\n\n\nQuarto vs R Markdown - Lequel utiliser?\nLes deux fonctionnent très bien! Les principales différences :\n\nR Markdown (.Rmd): Le format plus ancien et bien établi - nécessite le package rmarkdown\nQuarto (.qmd): Le format plus récent - fonctionne avec R, Python, Julia, et plus - ne nécessite pas de packages R supplémentaires\n\nPour les consultations, les deux formats conviennent! Utilisez celui avec lequel vous êtes à l’aise.\n\n\nComment créer un document Quarto/R Markdown\nDans RStudio:\n\nFile → New File → Quarto Document (ou R Markdown)\nDonnez-lui un titre et un auteur\nChoisissez HTML comme format de sortie\nCliquez sur “Create”\n\n\n\nComment générer en HTML\nUne fois que vous avez écrit votre document, vous devez le “rendre” pour créer la sortie HTML :\nMéthode 1: Cliquez sur le bouton Render/Knit - Quarto: Cliquez sur le bouton “Render” en haut de l’éditeur - R Markdown: Cliquez sur le bouton “Knit” en haut de l’éditeur\nMéthode 2: Utilisez la console\n# Pour les documents Quarto (.qmd)\nquarto::quarto_render(\"votre_fichier.qmd\")\n\n# Pour les documents R Markdown (.Rmd)\nrmarkdown::render(\"votre_fichier.Rmd\")\n\n\nQuoi m’envoyer\nLors du partage de votre analyse pour consultation, veuillez m’envoyer les deux :\n\nLe fichier source (.qmd ou .Rmd) - pour que je puisse voir votre code\nLe HTML rendu (.html) - pour que je puisse voir vos résultats sans exécuter le code\n\nDe cette façon, je peux examiner vos résultats et explorer votre code si nécessaire!\n\n\n\nConseils pour votre code\n✅ À FAIRE:\n\nCommencer avec une question de recherche claire\nCharger tous les packages en haut\nCommenter votre code abondamment\nVisualiser avant d’analyser\nVérifier les hypothèses du modèle\nInterpréter les résultats en langage simple\n\n❌ À NE PAS FAIRE:\n\nExécuter des analyses sans explorer les données d’abord\nOublier de vérifier les diagnostics du modèle\nLaisser du code sans commentaires\nIgnorer l’interprétation des résultats statistiques\nEnvoyer du code qui ne fonctionne pas!\n\nPour un exemple complet de code bien structuré, consultez le Tutoriel sur la structure du code."
  }
]